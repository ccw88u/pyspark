{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Demo - (In Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Demo - (In Zepplin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data\n",
    "a = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "# get even number\n",
    "b = []\n",
    "for ele in a:\n",
    "    if ele % 2 == 0: \n",
    "        b.append(ele)\n",
    "        \n",
    "# get sum of even number\n",
    "c = sum(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,5,6,7,8])\n",
    "\n",
    "def f(ele):\n",
    "    return ele % 2 == 0\n",
    "\n",
    "sum(a[f(a)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark 語法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5], 4)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data From File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeppelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark \n",
    "lines = sc.textFile('file:/tmp/trump.txt') \n",
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief Justice Roberts, President Carter, President Clinton, President Bush, fellow Americans and people of the world – thank you.',\n",
       " 'We the citizens of America have now joined a great national effort to rebuild our county and restore its promise for all our people.',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('trump.txt')\n",
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addNum(a, b):\n",
    "    return a + b\n",
    "\n",
    "addNum(2,3)\n",
    "\n",
    "addNum2 = lambda a, b : a + b\n",
    "addNum2(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = lambda e: e**2\n",
    "exp(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4]) \n",
    "a   = rdd.map(lambda x: x * 2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, False, True]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4]) \n",
    "a   = rdd.map(lambda x: x % 2 == 0)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a   = rdd.filter(lambda x: x % 2 == 0)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "a[a % 2 == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 2, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 4, 2,2,3]) \n",
    "a   = rdd.distinct()\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6], [2, 7], [3, 8]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3])\n",
    "a = rdd.map(lambda x:[x,x+5])\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 2, 7, 3, 8]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3])\n",
    "a = rdd.flatMap(lambda x:[x,x+5])\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([1,2,3]) \n",
    "rdd.reduce(lambda a,b:a*b)\n",
    "\n",
    "#  1   2   3\n",
    "#    2\n",
    "#       6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize([5,3,1,2]) \n",
    "rdd.takeOrdered(3,lambda s:-1*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1,2), (3,4), (3,6)]) \n",
    "a = rdd.reduceByKey(lambda a, b: a + b) \n",
    "a.collect()\n",
    "#RDD: [(1,2), (3,4), (3,6)] → [(1,2), (3,10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (1, 'b'), (2, 'c')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "a = rdd2.sortByKey()\n",
    "a.collect()\n",
    "#RDD: [(1,'a'), (2,'c'), (1,'b')] → [(1,'a'), (1,'b'), (2,'c')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x7f3ae4e73390>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x7f3ae4e73518>)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "rdd2.groupByKey()\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0) \n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "def f(x):\n",
    "    global accum \n",
    "    accum += x\n",
    "\n",
    "rdd.foreach(f) \n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 電影分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196\\t242\\t3\\t881250949',\n",
       " '186\\t302\\t3\\t891717742',\n",
       " '22\\t377\\t1\\t878887116',\n",
       " '244\\t51\\t2\\t880606923',\n",
       " '166\\t346\\t1\\t886397596']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"u.data\") \n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(242, 1), (302, 1), (377, 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies= lines.map(lambda x : (int(x.split()[1]) , 1) ) \n",
    "movies.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieCounts = movies.reduceByKey(lambda x,y: x+ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(242, 117), (302, 297), (346, 126)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieCounts.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = movieCounts.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 583), (258, 509), (100, 508), (181, 507), (294, 485)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open('u.item', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "            #print(fields[0], fields[1])\n",
    "            #break\n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameDict = sc.broadcast(loadMovieNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Star Wars (1977)', 583),\n",
       " ('Contact (1997)', 509),\n",
       " ('Fargo (1996)', 508),\n",
       " ('Return of the Jedi (1983)', 507),\n",
       " ('Liar Liar (1997)', 485),\n",
       " ('English Patient, The (1996)', 481),\n",
       " ('Scream (1996)', 478),\n",
       " ('Toy Story (1995)', 452),\n",
       " ('Air Force One (1997)', 431),\n",
       " ('Independence Day (ID4) (1996)', 429)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = movieCounts.sortBy(lambda a: -a[1])\n",
    "res2 = res.map(lambda e: (nameDict.value.get(e[0]), e[1]))\n",
    "res2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'ratings.txt'\n",
    "raw_data  = sc.textFile(data_file, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userid::itemid::rating', '0::0::4', '0::1::5']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userid::itemid::rating'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = raw_data.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_data = raw_data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0::0::4', '0::1::5', '0::7495::3']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '0', '4'], ['0', '1', '5'], ['0', '7495', '3']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data = skip_data.map(lambda l: l.split(\"::\"))\n",
    "csv_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(itemid='0', rating=4, userid='0'),\n",
       " Row(itemid='1', rating=5, userid='0'),\n",
       " Row(itemid='7495', rating=3, userid='0')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_data = csv_data.map(\n",
    "        lambda p: \n",
    "            Row(\n",
    "                userid=p[0], \n",
    "                itemid=p[1], \n",
    "                rating=int(p[2]) \n",
    "            )\n",
    ")\n",
    "row_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>rating</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemid  rating  userid\n",
       "0       0       4       0\n",
       "1       1       5       0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [{'userid':0, 'itemid':0, 'rating':4},\n",
    " {'userid':0, 'itemid':1, 'rating':5}\n",
    "]\n",
    "# padas df\n",
    "import pandas\n",
    "pandas_df = pandas.DataFrame(a)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark DataFrame\n",
    "df = sqlContext.createDataFrame(row_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(itemid='0', rating=4, userid='0'),\n",
       " Row(itemid='1', rating=5, userid='0'),\n",
       " Row(itemid='7495', rating=3, userid='0')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|itemid|rating|userid|\n",
      "+------+------+------+\n",
      "|     0|     4|     0|\n",
      "|     1|     5|     0|\n",
      "|  7495|     3|     0|\n",
      "+------+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|userid|       avg(rating)|\n",
      "+------+------------------+\n",
      "|   296|3.7885714285714287|\n",
      "|   467| 3.257575757575758|\n",
      "|   675| 3.193661971830986|\n",
      "|   691|3.6065573770491803|\n",
      "|   829|3.9507042253521125|\n",
      "|  1090| 3.398876404494382|\n",
      "|  1159|               4.4|\n",
      "|  1436| 4.101167315175097|\n",
      "|  1512| 3.466666666666667|\n",
      "|  1572|             3.895|\n",
      "|  2069|3.8285714285714287|\n",
      "|  2088|3.7857142857142856|\n",
      "|  2136|               3.6|\n",
      "|  2162| 2.857142857142857|\n",
      "|  2294|               3.0|\n",
      "|  2904| 4.222222222222222|\n",
      "|  3210| 4.229166666666667|\n",
      "|  3414| 4.416666666666667|\n",
      "|  3606|               4.0|\n",
      "|  3959| 4.538461538461538|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"userid\", \"rating\").groupBy(\"userid\").avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemid: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- userid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|userid|       avg(rating)|\n",
      "+------+------------------+\n",
      "|   296|3.7885714285714287|\n",
      "|   467| 3.257575757575758|\n",
      "|   675| 3.193661971830986|\n",
      "|   691|3.6065573770491803|\n",
      "|   829|3.9507042253521125|\n",
      "|  1090| 3.398876404494382|\n",
      "|  1159|               4.4|\n",
      "|  1436| 4.101167315175097|\n",
      "|  1512| 3.466666666666667|\n",
      "|  1572|             3.895|\n",
      "|  2069|3.8285714285714287|\n",
      "|  2088|3.7857142857142856|\n",
      "|  2136|               3.6|\n",
      "|  2162| 2.857142857142857|\n",
      "|  2294|               3.0|\n",
      "|  2904| 4.222222222222222|\n",
      "|  3210| 4.229166666666667|\n",
      "|  3414| 4.416666666666667|\n",
      "|  3606|               4.0|\n",
      "|  3959| 4.538461538461538|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_data = sqlContext.sql(\"\"\"\n",
    "    SELECT userid,AVG(rating) FROM ratings GROUP BY userid\n",
    "\"\"\") \n",
    "ratings_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieid='242', rating=3, userid='196'),\n",
       " Row(movieid='302', rating=3, userid='186'),\n",
       " Row(movieid='377', rating=1, userid='22'),\n",
       " Row(movieid='51', rating=2, userid='244')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = sc.textFile('file://tmp/u.data', 4) \n",
    "ratings_data = ratings.map(lambda l:l.split()) \n",
    "ratings_row_data = ratings_data.map(lambda p:\n",
    "    Row( userid=p[0], movieid=p[1], rating=int(p[2]) ) )\n",
    "\n",
    "ratings_row_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(ratings_row_data) \n",
    "df.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|movieid|count(1)|\n",
      "+-------+--------+\n",
      "|     50|     583|\n",
      "|    258|     509|\n",
      "|    100|     508|\n",
      "|    181|     507|\n",
      "|    294|     485|\n",
      "|    286|     481|\n",
      "|    288|     478|\n",
      "|      1|     452|\n",
      "|    300|     431|\n",
      "|    121|     429|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_data = sqlContext.sql(\"\"\"\n",
    "    SELECT movieid, count(1) FROM ratings GROUP BY movieid ORDER BY COUNT(1) DESC LIMIT 10\n",
    "\"\"\") \n",
    "ratings_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(movieid='1', moviename='Toy Story (1995)'),\n",
       " Row(movieid='2', moviename='GoldenEye (1995)'),\n",
       " Row(movieid='3', moviename='Four Rooms (1995)'),\n",
       " Row(movieid='4', moviename='Get Shorty (1995)')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = sc.textFile('file://tmp/u.item', 4)\n",
    "movies_data = movies.map(lambda l:l.split('|')) \n",
    "#movies_data.take(3)\n",
    "movies_row_data = movies_data.map(lambda p:\n",
    "    Row(movieid=p[0], moviename=p[1] ) )\n",
    "movies_row_data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = sqlContext.createDataFrame(movies_row_data)\n",
    "movies_df.registerTempTable(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+\n",
      "|           moviename|avg_rating|cnt|\n",
      "+--------------------+----------+---+\n",
      "|Someone Else's Am...|       5.0|  1|\n",
      "|Saint of Fort Was...|       5.0|  2|\n",
      "|Aiqing wansui (1994)|       5.0|  1|\n",
      "|Marlene Dietrich:...|       5.0|  1|\n",
      "|     Star Kid (1997)|       5.0|  3|\n",
      "+--------------------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_movies = sqlContext.sql(\"\"\"\n",
    "SELECT moviename,avg(rating) as avg_rating, count(1) as cnt \n",
    "FROM movies INNER JOIN ratings ON ratings.movieid = movies.movieid \n",
    "GROUP BY moviename \n",
    "ORDER by AVG(rating) DESC LIMIT 10\n",
    "\"\"\") \n",
    "best_movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+---+\n",
      "|           moviename|        avg_rating|cnt|\n",
      "+--------------------+------------------+---+\n",
      "|    Star Wars (1977)|4.3584905660377355|583|\n",
      "|      Contact (1997)|3.8035363457760316|509|\n",
      "|        Fargo (1996)| 4.155511811023622|508|\n",
      "|Return of the Jed...| 4.007889546351085|507|\n",
      "|    Liar Liar (1997)| 3.156701030927835|485|\n",
      "+--------------------+------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_movies = sqlContext.sql(\"\"\"\n",
    "SELECT moviename,avg(rating) as avg_rating, count(1) as cnt \n",
    "FROM movies INNER JOIN ratings ON ratings.movieid = movies.movieid \n",
    "GROUP BY moviename \n",
    "ORDER by cnt DESC LIMIT 10\n",
    "\"\"\") \n",
    "best_movies.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
